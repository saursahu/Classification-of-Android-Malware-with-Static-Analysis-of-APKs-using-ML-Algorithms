# -*- coding: utf-8 -*-
"""
Created on Sun Oct 20 22:51:37 2019

@author: praty
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('./saurabh_dataset.csv')
dataset= dataset.dropna()

X = dataset.iloc[:, 0:24].values
X = X.astype('int64')
y = dataset.iloc[:, 27].values
y = y.astype('int64')
y = y.reshape(y.shape[0], 1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Fitting KNN Classification to the Training set
from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 3)
classifier.fit(X_train, y_train)

acc_knn = round(classifier.score(X_train, y_train) * 100, 2)
print('Accuracy: %.2f' % (acc_knn))

# Predicting the Test set results
y_pred = classifier.predict(X_test)
predictions = classifier.predict(X_train)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
cm

# 10-fold cross validation
from sklearn.model_selection import cross_val_score
scores = cross_val_score(classifier, X_train, y_train, cv=10, scoring = "accuracy")
print("Scores:", scores)
print("Mean:", np.round(scores.mean(),8))
print("Standard Deviation:", np.round(scores.std(),8))

# precision, recall and f1 score
from sklearn.metrics import precision_score, recall_score, f1_score
print("Precision:", precision_score(y_train, predictions))
print("Recall:",recall_score(y_train, predictions))
print("F1 score:",f1_score(y_train, predictions))

# precision-recall curve
from sklearn.metrics import precision_recall_curve
y_scores = classifier.predict_proba(X_train)
y_scores = y_scores[:,1]

precision, recall, threshold = precision_recall_curve(y_train, y_scores)

with open('knn_recall.txt', 'w') as filehandle:
    for listitem in recall:
        filehandle.write('%.6f\n' % listitem)

def plot_precision_and_recall(precision, recall, threshold):
    plt.plot(threshold, precision[:-1], "r-", label="precision", linewidth=5)
    plt.plot(threshold, recall[:-1], "b", label="recall", linewidth=5)
    plt.xlabel("threshold", fontsize=19)
    plt.legend(loc="upper right", fontsize=19)
    plt.ylim([0, 1])

plt.figure(figsize=(14, 7))
plot_precision_and_recall(precision, recall, threshold)
plt.show()


# roc_curve
from sklearn.metrics import roc_curve
false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, y_scores)


with open('knn_false.txt', 'w') as filehandle:
    for listitem in false_positive_rate:
        filehandle.write('%.6f\n' % listitem)


def plot_roc_curve(false_positive_rate, true_positive_rate, label=None):
    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)
    plt.plot([0, 1], [0, 1], 'r', linewidth=4)
    plt.axis([0, 1, 0, 1])
    plt.xlabel('False Positive Rate (FPR)', fontsize=16)
    plt.ylabel('True Positive Rate (TPR)', fontsize=16)

plt.figure(figsize=(14, 7))
plot_roc_curve(false_positive_rate, true_positive_rate)
plt.show()

